{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO25wEvCJ/RG9ODBtzccA84",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chilik78/Lab_01/blob/main/Lab_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Подгрузка файлов в Google Colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "uploaded"
      ],
      "metadata": {
        "id": "RCQJNIzyjIX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Просмотр содержимого в папке /content/\n",
        "import os\n",
        "# выводим пути к папкам (dirpath) и наименования файлов (filenames) и после этого\n",
        "\n",
        "for dirpath, _, filenames in os.walk('/content/'):\n",
        "  for filename in filenames:# во вложенном цикле проходимся по названиям файлов\n",
        "    print(os.path.join(dirpath, filename))# и соединяем путь до папок и входящие в эти папки файлы с помощью метода path.join()\n",
        "\n",
        "print(\"\\n\")\n",
        "!ls \n",
        "print(\"\\n\")\n",
        "!ls /content/sample_data/"
      ],
      "metadata": {
        "id": "YYSIbV2r9aeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Чтение из переменной uploaded\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "# посмотрим на тип значений словаря uploaded\n",
        "type(uploaded['test.csv'])\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "uploaded_str = uploaded['test.csv'].decode()\n",
        "print(type(uploaded_str))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(uploaded_str[:35])\n",
        "\n",
        "print(\"\\n\")\n",
        "uploaded_list = uploaded_str.split('\\r\\n')\n",
        "type(uploaded_list)\n",
        "print(\"\\n\")\n",
        "\n",
        "# не забудем создать индекс с помощью функции enumerate()\n",
        "\n",
        "for i, line in enumerate(uploaded_list):\n",
        "  # начнем выводить записи\n",
        "  print(line)\n",
        "  print(\"\\n\")\n",
        "  \n",
        "  # когда дойдем до четвертой строки\n",
        "  if i == 3:\n",
        "    # прервемся\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "juKYeYlW_Rir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Использование функции open() и конструкции with open()\n",
        "# передадим функции open() адрес файла\n",
        "# параметр 'r' означает, что мы хотим прочитать (read) файл\n",
        "f1 = open('/content/train.csv', 'r')\n",
        "\n",
        "# метод .read() помещает весь файл в одну строку\n",
        "# выведем первые 142 символа (если параметр не указывать, выведется все содержимое)\n",
        "print(f1.read(142))\n",
        "\n",
        "# в конце файл необходимо закрыть\n",
        "f1.close()\n",
        "\n",
        "# снова откроем файл\n",
        "f2 = open('/content/train.csv', 'r')\n",
        "\n",
        "# пройдемся по нашему объекту в цикле for и параллельно создадим индекс\n",
        "for i, line in enumerate(f2):\n",
        "# выведем строки без служебных символов по краям\n",
        "  print(line.strip())\n",
        "  # дойдя до четвертой строки, прервемся\n",
        "  if i == 3:\n",
        "    break\n",
        "# не забудем закрыть файл\n",
        "f2.close()\n",
        "\n",
        "# скажем Питону: \"открой файл и назови его f3\"\n",
        "with open('/content/test.csv', 'r') as f3:\n",
        "# \"пройдись по строкам без служебных символов\"\n",
        "  for i, line in enumerate(f3):\n",
        "    print(line.strip())\n",
        "# и \"прервись на четвертой строке\"\n",
        "    if i == 3:\n",
        "      break\n",
        "\n"
      ],
      "metadata": {
        "id": "k7P7FMb9EeSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# импортируем библиотеку\n",
        "import pandas as pd\n",
        "# применим функцию read_csv() и посмотрим на первые три записи файла train.csv\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "train.head(3)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# сделаем то же самое с файлом test.csv\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "test.head(3)\n",
        "\n"
      ],
      "metadata": {
        "id": "jb62V7UBGwlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Обработка и анализ данных. Исследовательский анализ данных (EDA)\n",
        "train.info()\n",
        "print(\"\\n\")\n",
        "\n",
        "# для построения графиков воспользуемся новой для нас библиотекой seaborn\n",
        "import seaborn as sns\n",
        "\n",
        "# посмотрим насколько значим класс билета длявыживания пассажира\n",
        "# с помощью x и hue мы можем уместить две категориальные переменные на одном графике\n",
        "sns.countplot(x = 'Pclass', hue = 'Survived', data = train)\n",
        "print(\"\\n\")\n",
        "\n",
        "# кто выживал чаще, мужчины или женщины?\n",
        "sns.countplot(x = 'Sex', hue = 'Survived', data = train)\n",
        "print(\"\\n\")\n",
        "\n",
        "# выявим пропущенные значения с помощью .isnull() и посчитаем их количество sum()\n",
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "hlDZx83oIThg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# переменная Cabin (номер каюты), скорее всего, не является самой важной\n",
        "# избавимся от нее с помощью метода .drop()\n",
        "# (параметр axis отвечает за столбцы, inplace = True сохраняет изменения)\n",
        "#train.drop(columns = 'Cabin', axis = 1, inplace = True)\n",
        "\n",
        "# а вот Age (возраст) точно важен, заменим пустые значения средним арифметическим\n",
        "train['Age'].fillna(train['Age'].mean(), inplace = True)\n",
        "\n",
        "# у нас остаются две пустые строки в Embarked, удалим их\n",
        "train.dropna(inplace = True)\n",
        "\n",
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "cy5cv5nsjHrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Категориальные переменные\n",
        "# применим one-hot encoding к переменной Sex (пол) с помощью метода .get_dummies()\n",
        "\n",
        "pd.get_dummies(train['Sex']).head(3)\n",
        "sex.head(3)\n",
        "# удалим первый столбец, он избыточен\n",
        "sex = pd.get_dummies(train['Sex'], drop_first = True)\n",
        "#sex.head(3)\n",
        "\n",
        "embarked = pd.get_dummies(train['Embarked'], drop_first = True)\n",
        "pclass = pd.get_dummies(train['Pclass'], drop_first = True)\n",
        "\n",
        "train = pd.concat([train, pclass, sex, embarked], axis = 1)\n",
        "train.head(3)"
      ],
      "metadata": {
        "id": "4iSVp_a_mKgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Отбор признаков\n",
        "\n",
        "# применим функцию drop() к соответствующим столбцам\n",
        "train.drop(['PassengerId', 'Pclass', 'Name', 'Sex', 'Ticket', 'Embarked'], axis = 1, inplace = True)\n",
        "train.head(3)\n",
        "\n"
      ],
      "metadata": {
        "id": "vf1gyc7fpO7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# импортируем класс StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# создадим объект этого класса\n",
        "scaler = StandardScaler()\n",
        "# выберем те столбцы, которые мы хотим масштабировать\n",
        "cols_to_scale = ['Age', 'Fare']\n",
        "# рассчитаем среднее арифметическое и СКО для масштабирования данных\n",
        "scaler.fit(train[cols_to_scale])\n",
        "# применим их\n",
        "train[cols_to_scale] = scaler.transform(train[cols_to_scale])\n",
        "# посмотрим на результат\n",
        "train.head(3)\n",
        "\n",
        "train.columns = train.columns.map(str)"
      ],
      "metadata": {
        "id": "aynVzJHCqojE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Разделение обучающей выборки на признаки и целевую переменную\n",
        "# поместим в X_train все кроме столбца Survived\n",
        "X_train = train.drop('Survived', axis = 1)\n",
        "# столбец 'Survived' станет нашей целевой переменной (y_train)\n",
        "y_train = train['Survived']\n",
        "X_train.head(3)"
      ],
      "metadata": {
        "id": "Aih3hY2zr6Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Обучение модели логистической регрессии\n",
        "# импортируем логистическую регрессию из модуля linear_model библиотеки sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# создадим объект этого класса и запишем его в переменную model\n",
        "model = LogisticRegression()\n",
        "# обучим нашу модель\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# сделаем предсказание класса на обучающей выборке\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "# построим матрицу ошибок\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# передадим ей фактические и прогнозные значения\n",
        "conf_matrix = confusion_matrix(y_train, y_pred_train)\n",
        "# преобразуем в датафрейм\n",
        "conf_matrix_df = pd.DataFrame(conf_matrix)\n",
        "conf_matrix_df\n",
        "\n",
        "#Для удобства интерпретации добавим подписи.\n",
        "conf_matrix_labels = pd.DataFrame(conf_matrix, columns = ['Прогноз погиб', 'Прогноз выжил'], index = ['Факт погиб', 'Факт выжил'])\n",
        "conf_matrix_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0uCd72PsQGV",
        "outputId": "e63ed85d-8045-432d-cdac-0ffabf5e2a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# рассчитаем метрику accuracy вручную\n",
        "round((478 + 237)/(478 + 237 + 71 + 103), 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXZyy-CduDVc",
        "outputId": "5bebacac-8ee3-410a-f4b2-88d8b5f95ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.804"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Построение прогноза на тестовых данных\n",
        "test.info()\n",
        "print(\"\\n\")\n",
        "# для начала дадим датасету привычное название X_test\n",
        "X_test = test\n",
        "\n",
        "# заполним пропуски в переменных Age и Fare средним арифметическим\n",
        "X_test['Age'].fillna(test['Age'].mean(), inplace = True)\n",
        "X_test['Fare'].fillna(test['Fare'].mean(), inplace = True)\n",
        "\n",
        "# выполним one-hot encoding категориальных переменных\n",
        "sex = pd.get_dummies(X_test['Sex'], drop_first = True)\n",
        "embarked = pd.get_dummies(X_test['Embarked'], drop_first = True)\n",
        "pclass = pd.get_dummies(X_test['Pclass'], drop_first = True)\n",
        "\n",
        "# присоединим новые столбцы к исходному датафрейму\n",
        "X_test = pd.concat([test, pclass, sex, embarked], axis = 1)\n",
        "# и удалим данные, которые теперь не нужны\n",
        "X_test.drop(['PassengerId', 'Pclass', 'Name', 'Sex','Cabin', 'Ticket', 'Embarked'], axis = 1, inplace = True)\n",
        "# посмотрим на результат\n",
        "X_test.head(3)\n",
        "\n",
        "# применим среднее арифметическое и СКО обучающей выборки для масштабирования тестовых данных\n",
        "X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
        "X_test.columns = X_test.columns.map(str)#Остается превратить название столбцов в строки.\n",
        "y_pred_test = model.predict(X_test) #И сделать прогноз на тестовой выборке.\n",
        "# посмотрим на первые 10 прогнозных значений\n",
        "y_pred_test[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "mMSMhSLXujyD",
        "outputId": "7a80b2b1-0fee-4b17-ff04-f712fd0bb307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 418 entries, 0 to 417\n",
            "Data columns (total 11 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  418 non-null    int64  \n",
            " 1   Pclass       418 non-null    int64  \n",
            " 2   Name         418 non-null    object \n",
            " 3   Sex          418 non-null    object \n",
            " 4   Age          418 non-null    float64\n",
            " 5   SibSp        418 non-null    int64  \n",
            " 6   Parch        418 non-null    int64  \n",
            " 7   Ticket       418 non-null    object \n",
            " 8   Fare         418 non-null    float64\n",
            " 9   Cabin        91 non-null     object \n",
            " 10  Embarked     418 non-null    object \n",
            "dtypes: float64(2), int64(4), object(5)\n",
            "memory usage: 36.0+ KB\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
            "Feature names must be in the same order as they were in fit.\n",
            "\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-193-0539f1c7771e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_to_scale\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_to_scale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Остается превратить название столбцов в строки.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#И сделать прогноз на тестовой выборке.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m# посмотрим на первые 10 прогнозных значений\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mVector\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             raise ValueError(\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: X has 9 features, but LogisticRegression is expecting 68 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Сохранение результата в новом файле на сервере\n",
        "# файл с примером можно загрузить не с локального компьютера, а из Интернета\n",
        "url = 'https://www.dmitrymakarov.ru/wp-content/uploads/2021/11/titanic_example.csv'\n",
        "# просто поместим его url в функцию read_csv()\n",
        "example = pd.read_csv(url)\n",
        "example.head(3)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# возьмем индекс пассажиров из столбца PassengerId тестовой выборки\n",
        "ids = test['PassengerId']\n",
        "# создадим датафрейм из словаря, в котором\n",
        "# первая пара ключа и значения - это id пассажира, вторая - прогноз \"на тесте\"\n",
        "result = pd.DataFrame({'PassengerId': ids, 'Survived': y_pred_test})\n",
        "# посмотрим, что получилось\n",
        "result.head()\n",
        "\n",
        "# создадим новый файл result.csv с помощью функции to_csv(), удалив при этом индекс\n",
        "result.to_csv('result.csv', index = False)\n",
        "# файл будет сохранен в 'Сессионном хранилище' и, если все пройдет успешно, выведем следующий текст:\n",
        "print('Файл успешно сохранился в сессионное хранилище!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "dpuJu3hHw6VT",
        "outputId": "5c9419e4-9e39-4550-c76c-c2f2dfe33e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-166-1a02b0c86671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# создадим датафрейм из словаря, в котором\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# первая пара ключа и значения - это id пассажира, вторая - прогноз \"на тесте\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'PassengerId'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Survived'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# посмотрим, что получилось\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Скачивание обратно на жесткий диск\n",
        "# применим метод .download() объекта files\n",
        "files.download('/content/result.csv')"
      ],
      "metadata": {
        "id": "yjlFR5GtzIq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}